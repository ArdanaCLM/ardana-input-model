<h2 id="ardana-single-region-entry-scale-cloud-with-kvm-esx-hypervisor-and-external-ses-integration-example">Ardana Single regioon Entry Scale Cloud with KVM &amp; ESX Hypervisor and External SES integration Example</h2>
<p>The input files in this example deploy a cloud that has the following characteristics:</p>
<h3 id="control-planes">Control Planes</h3>
<ul>
<li>A single control plane consisting of three servers that co-host all of the required services.</li>
</ul>
<h3 id="resource-pools">Resource Pools</h3>
<ul>
<li>Compute:
<ul>
<li>KVM: Runs nova computes and associated services. Runs on nodes of role type COMPUTE-ROLE. The example lists 1 node.</li>
<li>ESX: (below listed resources will be provisioned for all activated clusters)
<ul>
<li>One instance of Nova compute Proxy per cluster</li>
<li>One instance of OVSvApp per node</li>
</ul></li>
</ul></li>
</ul>
<p><em>User shall add required information related to compute proxy and OVSvApp Nodes</em></p>
<p><em>Additional resource nodes can be added to the configuration.</em></p>
<p><em>Minimal Swift Resources are provided by the control plane</em></p>
<h3 id="deployer-node">Deployer Node</h3>
<p>This configuration runs the lifecycle-manager (formerly referred to as the deployer) on a control plane node. You need to include this node address in your servers.yml definition. This function does not need a dedicated network.</p>
<p><em>The minimum server count for this example is therefore 4 servers (Control Plane (x3) + 1 activated vCenter cluster having atleast 1 host)</em></p>
<p>An example set of servers are defined in <strong><em>data/servers.yml</em></strong>. You will need to modify this file to reflect your specific environment.</p>
<h3 id="networking">Networking</h3>
<p>The example requires the following networks:</p>
<p>IPMI/iLO network, connected to the lifecycle-manager and the IPMI/iLO ports of all servers</p>
<p>A pair of bonded NICs which are used by the following networks:</p>
<ul>
<li>External API - This is the network that users will use to make requests to the cloud</li>
<li>External VM - This is the network that will be used to provide access to VMs (via floating IP addresses)</li>
<li>Guest - This is the network that will carry traffic between VMs on private networks within the cloud</li>
<li>Cloud Management - This is the network that will be used for all internal traffic between the cloud services, This network is also used to install and configure the nodes. This network needs to be on an untagged VLAN</li>
<li>SES - This is the network that control plane and compute nodes clients will use to talk to the external SES</li>
<li>TRUNK - This is the network that will be used on OVSvApp service VM</li>
<li>ESX-CONF - This is the network that will be used to provision the OS onto the nodes and to perform the initial OS configuration</li>
</ul>
<p>Note that the EXTERNAL-API network must be reachable from the EXTERNAL-VM network if you want VMs to be able to make API calls to the cloud and user can choose bonded nic or dedicated nic and can feed VLANs to any network interface based on the nic availability.</p>
<p>TRUNK network is the network that will be used to apply security group rules on tenant traffic. It is managed internally by Ardana cloud and is restricted to the vCenter environment.</p>
<p>ESX-CONF-NET network (of ESX-CONF network-group) represents a network that is used only to configure the ESX compute nodes in the cloud. This deployer network should be different from the pxe-based deployer network used by cobbler to standup the cloud controller cluster.</p>
<p>The Data Center Management network (which hosts the vcenter server) must be reachable from the Cloud Management network so that the controllers, compute proxy and OVSvApp nodes can communicate to the vcenter server.</p>
<p>An example set of networks are defined in <strong><em>./data/networks.yml</em></strong>. You will need to modify this file to reflect your environment.</p>
<p>The example uses the devices hed3 &amp; hed4 as a bonded network for all services. If you need to modify these for your environment they are defined in <strong><em>./data/net_interfaces.yml</em></strong> The network devices eth3 &amp; eth4 are renamed to devices hed4 &amp; hed5 using the PCI bus mappings secified in <strong><em>./data/nic_mappings.yml</em></strong>. You may need to modify the PCI bus addresses to match your system.</p>
<h3 id="local-storage">Local Storage</h3>
<p>All servers should present a single OS disk, protected by a RAID controller. This disk needs to be at least 512GB in capacity. In addition the example configures one additional disk depending on the role of the server:</p>
<ul>
<li>Controllers: /dev/sdb is configured to be used by Swift</li>
<li>Compute Severs: /dev/sdb is configured as an additional Volume Group to be used for VM storage</li>
</ul>
<p>Additional discs can be configured for any of these roles by editing the corresponding <strong><em>./data/disks_</em>.yml</strong>* file</p>

<footer>
  <small>(c) Copyright 2015 Hewlett Packard Enterprise Development LP </small>
  <small>(c) Copyright 2018 SUSE LLC </small>
  <small>Licensed under the Apache License, Version 2.0 (the "License"); you may </small>
  <small>not use this file except in compliance with the License. You may obtain </small>
  <small>a copy of the License at<br><br></small>
  <small>http://www.apache.org/licenses/LICENSE-2.0<br><br></small>
  <small>Unless required by applicable law or agreed to in writing, software </small>
  <small>distributed under the License is distributed on an "AS IS" BASIS, WITHOUT </small>
  <small>WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the </small>
  <small>License for the specific language governing permissions and limitations </small>
  <small>under the License.<br></small>
</footer>
